# vllm engine args reference: https://docs.vllm.ai/en/latest/configuration/engine_args.html

models:
  - 
    name: "kokoro"
    model: "hexgrad/Kokoro-82M"
    usecase: "tts"
    plugin: "kokoro"
  - 
    name: "whisper"
    model: "openai/whisper-small"
    # use_vllm: false
    usecase: "transcription"
    vllm_engine_kwargs:
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.15
      trust_remote_code: 1
  - 
    name: "nomic-embed-text-v1.5"
    model: "nomic-ai/nomic-embed-text-v1.5"
    usecase: "embed"
    vllm_engine_kwargs:
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.05
      trust_remote_code: 1
  # - 
  #   name: "bark-small"
  #   model: "suno/bark-small"
  #   usecase: "tts"
  #   plugin: "bark"
  #   transformers_config:
  #     device: "cuda:0"
  # - 
  #   name: "orpheus"
  #   model: "unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit"
  #   usecase: "tts"
  #   plugin: "orpheus"
  #   vllm_engine_kwargs:
  #     tensor_parallel_size: 1
  #     gpu_memory_utilization: 0.5
  #     trust_remote_code: 1
  #     max_model_len: 2048
  #     dtype: float16
  #     kv_cache_dtype: "fp8_e4m3"
  #     quantization: "fp8"
  # -
  #   name: "gemma-3n-E4B"
  #   model: "unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit"
  #   usecase: "generate"
  #   vllm_engine_kwargs:
  #     tensor_parallel_size: 1
  #     max_model_len: 32000
  #     gpu_memory_utilization: 0.6
  #     enable_log_requests: 1
  #     kv_cache_dtype: "fp8"
  -
    name: "Qwen3-4B"
    model: "unsloth/Qwen3-4B-unsloth-bnb-4bit"
    usecase: "generate"
    vllm_engine_kwargs:
      tensor_parallel_size: 1
      max_model_len: 32000
      gpu_memory_utilization: 0.60
      enable_log_requests: 1
  
  
